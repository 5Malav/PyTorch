# -*- coding: utf-8 -*-
"""PyTorch Gradient descent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fm0A7zlqELzXdaNpDBrk_OBF-3SJA48t

#PyTorch Gradient descent

using Auto Gradient function
"""

import torch

"""
Function:- 2x^4 + x^3 + 3x^2 + 5x + 1

Derivative:- 8x^3 + 3x^2 + 6x + 5

calculate gradient of Function

"""

x= torch.tensor(2.0,requires_grad=True)
x

# Thise Sets up computational tracking on the tensor.
# So now PyTorch can keep an eye on this tensor throughout
# the computational graph.
# It's notm just floating around as a separate variable.

# Define function

y= 2*x**4 + x**3 + 3*x**2 + 5*x + 1
y

# This is the value of y when x=2

type(y)

y.backward()

# This is going to perform the backpropagation

x.grad

# torch(93.)
# 93 is the representation of if you were to plug in
# the X value into the 1st derivative of Y
# with respect to X.
# So 93 is gonna be the slope of the polynomial
# at that point where X=2.

# This is the slope of the polynomial at the point (2,63)

"""#Back -  Propagation on multiple steps

 Now let's do something more complex, involving layers y and z between x and out out layer out
"""

x= torch.tensor([[1.,2.,3.],[3.,2.,1.]],requires_grad=True)
x

# Create 1st layer

y = 3*x +2
y

z = 2*y**2
z

out =  z.mean()
out

# Perform back propagation to find the gradient of X
# with respect to this output layer

out.backward()

x.grad