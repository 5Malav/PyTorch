# -*- coding: utf-8 -*-
"""Project - 2 Artificial Neural Nework with PyTorch .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-OwtECYaoc1DYv2i08-YTXxdE1H4tINZ

# Aritificial Neural Network to create a predictive algorithm

Data :- Iris Dataset

Iris data set contain continuous numberical data for 3 classes of flowers
1.) Iris Versicolor
2.) Iris Setosa
3.) Iris Virginica

Based of the Sepal and Petal lengths
and width we can predict flower species.

Our goal is to develop a model capable of classifying an iris plant based on four features. This is a multi-class classification where each sample can belong to ONE of 3 classes (<em>Iris setosa</em>, <em>Iris virginica</em> or <em>Iris versicolor</em>). The network will have 4 input neurons (flower dimensions) and 3 output neurons (scores). Our loss function will compare the target label (ground truth) to the corresponding output score.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import torch
import torch.nn as nn
import torch.nn.functional as F

from sklearn.model_selection import train_test_split

class Model(nn.Module):

    def __init__(self,input_features=4,h1=8,h2=9,output_features=3):

        # How many layers?
        # For each layer we create an attribute
        # And we decide on the kind of layer while
        # we are creating this.

        # Input Layer(4 features of iris dataset) --> h1 N(Nuerons)
        # --> h2 N Neurons --> Output(3 classes)
        # at least number of neurons should be equal to number of features

        # lets construct this layers
        # First we should instantiate the module that we inherited from
        # nn.Module should be called in inherited module
        # call its own init method

            super().__init__()

            self.fc1 = nn.Linear(input_features,h1)
            # fc1 mean fully connected layer

            self.fc2 = nn.Linear(h1,h2)
            #output of h1 from fc1 will be input  in h1 of fc2

            self.out= nn. Linear(h2,output_features)

            # we can add more layer/ play with number of neurons


    # set our forward propagation method


    def forward(self,x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.out(x)
        return x

"""
class Model(nn.Module):
    def __init__(self, in_features=4, h1=8, h2=9, out_features=3):
        super().__init__()
        self.fc1 = nn.Linear(in_features,h1)    # input layer
        self.fc2 = nn.Linear(h1, h2)            # hidden layer
        self.out = nn.Linear(h2, out_features)  # output layer

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.out(x)
        return x
        """

# Let's instantiate the model using the parameter defaults

torch.manual_seed(32)

model = Model()
model

df = pd.read_csv("iris.csv")
df.head()

df.tail()

fig,axes = plt.subplots(nrows=2,ncols=2,figsize=(10,7))

#This line creates a figure with a 2x2 grid of subplots,
#setting the figure size to 10x7 inches.
#The fig variable represents the entire figure,
#while axes is an array of the individual subplot objects.

fig.tight_layout()

#This line adjusts the spacing between subplots to
#prevent labels and titles from overlapping.

plots= [(0,1),(2,3),(0,2),(1,3)] # combinations of each column
colors=['b','r','g']
labels=['Iris setosa','Iris virginica','Iris versicolor']

"""
These lines define lists for plot indices, colors, and labels.

plots: A list of tuples, where each tuple represents the
       indices of columns to be plotted in each subplot.

colors: A list of colors for the different data classes.

labels: A list of labels for the legend, corresponding to the different data classes.
"""

for i,ax in enumerate(axes.flat):
    for j in range(3):

        x = df.columns[plots[i][0]]
        y = df.columns[plots[i][1]]

        ax.scatter(df[df['target']==j][x],\
                   df[df['target']==j][y],
                    color=colors[j])
        ax.set(xlabel=x,ylabel=y)

"""
This is a nested loop that iterates through each subplot and each data class (target value).

for i,ax in enumerate(axes.flat): loops through each subplot in the axes array.

for j in range(3): loops through the different target values (0, 1, and 2).

x = df.columns[plots[i][0]] and y = df.columns[plots[i][1]] get
the column names for the x and y axes based on the plots list.

ax.scatter(...) creates a scatter plot of the data points where
target equals j, using the specified color.

ax.set(xlabel=x,ylabel=y) sets the labels for the x and y axes.

"""
fig.legend(labels=labels,loc=3,bbox_to_anchor=(1.0,0.85))
plt.show()

#fig.legend(...) adds a legend to the figure using the labels defined earlier.

#plt.show() displays the plot.

X= df.drop('target',axis=1)
y= df['target']

X = X.values
y = y.values

X

y

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=33)

# covert in FloatTensor

X_train = torch.FloatTensor(X_train)

X_test = torch.FloatTensor(X_test)

y_train = torch.LongTensor(y_train)

y_test = torch.LongTensor(y_test)

criterion = nn.CrossEntropyLoss()

# criteria we are using in order to measure how far
# off your predictions are from your data
# cross entropy for multi class classifcation problem


optimizer = torch.optim.Adam(model.parameters(),lr=0.01)

model.parameters()

model.parameters

# Train model

# Epoch ?
# start with small number if dataset is large
# then plot loss function to decide number of epchs

# Epoch :- 1 run through all the training data

epochs = 100
losses = []

for i in range(epochs):
    i+=1
    y_pred = model.forward(X_train)
    loss = criterion(y_pred, y_train)
    losses.append(loss)

    # a neat trick to save screen space:
    if i%5 == 0:
        print(f'epoch: {i} and  loss: {loss}')

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# is 100 Epoch enough?
# lets plot epcoch vs loss function
plt.plot(range(epochs),[loss.detach().numpy() for loss in losses])
plt.ylabel("Loss")
plt.xlabel("Epoch")

# Somewhere between 80- 100 we are really starting to converge

# Validate the model on test data

with torch.no_grad():
# this going to impact the auto gradient engine and deactivate it.
# telling PyTorch that do not worry about backpropagation because
# we are just evaluating our model.
# Don't need to go back abd change those weights based on test data

    y_eval= model.forward(X_test)
    loss=criterion(y_eval,y_test)

loss

# similar loss value as training data set
# indicates good performance.
# it is not overfitting on training data. if that is the case
# then when we evaluate test data loss would be huge

correct = 0

# how many flowers do we actually evaluate correctly

with torch.no_grad():

    for i,data in enumerate(X_test):

        y_val = model.forward(data)

        # print 1.), 2.)
        # y_val is tensor so to print covert in string

        print(f'{i+1}. )  {str(y_val)}  -> {y_test[i]}')

        print(f'{i+1}. )  Predicted Class:- {str(y_val.argmax().item())} | Actual Class:-  {y_test[i]}')
        # the highest value indicates that network thinks
        # likelihood that the nework thinks that class it belongs to.
        # y_test[i] will print class it belongs to.

        # We have used CrossEntropyLoss, it ends up producing for us
        # at the end of the day is this tensor
        # tensor([-2.1252,  4.8064, -0.8628])  -> 1
        # call that last output layer had 3 values
        # and the greatest value, its going to be at the index
        # position of what class the netowrk thinks that
        # particular flower belongs in.

        # count how many are correct
        # print max value using argmax() from y_val values
        # and compare it with test data
        if y_val.argmax().item() == y_test[i]:
            correct+=1



print(f"We got {correct} correct!")

# Save the train model to file

torch.save(model.state_dict(),'my_iris_model_Project_2.pt')

new_model = Model()

# this model does not have any idea about the weights or the biases
# should be.

new_model.load_state_dict(torch.load("my_iris_model_Project_2.pt"))

new_model.eval()

# If we want to actually save the entire model
# including its class and parameters
# we have to save it as a pickle file
# So in this case instead of passing in the state dictionary
# we pass model directly.

new_flower = torch.tensor([5.6,3.7,2.2,0.5])

fig,axes = plt.subplots(nrows=2,ncols=2,figsize=(10,7))


fig.tight_layout()


plots= [(0,1),(2,3),(0,2),(1,3)] # combinations of each column
colors=['b','r','g']
labels=['Iris setosa','Iris virginica','Iris versicolor']


for i,ax in enumerate(axes.flat):
    for j in range(3):

        x = df.columns[plots[i][0]]
        y = df.columns[plots[i][1]]

        ax.scatter(df[df['target']==j][x],\
                   df[df['target']==j][y],
                    color=colors[j])
        ax.set(xlabel=x,ylabel=y)

    # Add a plot for our new flower
    ax.scatter(new_flower[plots[i][0]],new_flower[plots[i][1]],color='yellow')

fig.legend(labels=labels,loc=3,bbox_to_anchor=(1.0,0.85))
plt.show()

# Iris setosa is our new flower

with torch.no_grad():

    print(new_model(new_flower))

    print(new_model(new_flower).argmax())