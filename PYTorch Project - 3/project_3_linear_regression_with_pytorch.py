# -*- coding: utf-8 -*-
"""Project - 3  Linear Regression with PyTorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pdUrZ3pO50erw6Kq9jUiPuUwlvm06hrk
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import torch.nn as nn

# column matrix of X values

X= torch.linspace(1,50,50)
X

X= torch.linspace(1,50,50).reshape(-1,1)
X

# Create a random array of error values

torch.manual_seed(71)
e = torch.randint(-8,9,(50,1),dtype=torch.float)
e

# 50 rows and 1 column

y = 2*X +1+e #(y = ,x +b)
y

y.shape

# to print this we can't print PyTorch tensor
# we need to convert this in NumPy array

X.numpy()

plt.scatter(X.numpy(),y.numpy())

# From graph it can be see that there is a little bit of noise
# but the general formula is still 2X + 1
# So our slope is 2 and our Y intercept is 1.
# what we wanna do is we wanna be able to use PyTorch
# and fit a Linear Regression line here.
# So note that when we created tensor X, we did not pass
# requires_grad or gradient = True.
# This mean that right now this Y does not have a gradient
# function and so Y.bacword() will not actually work.
# Since PyTorch is not tracking operations it does not actually
# know the relationship between the variable Y and X.
# So keep that in mind this is just a linear regresssion model
# right now.
# we are not keeping tracke of any gradient so far.

torch.manual_seed(59)

model = nn.Linear(in_features=1,out_features=1)
print(model.weight)
print(model.bias)

# 0.1060 is our 1st random weight value
# 0.96 is our 1st random bias value

#PyTorch lets us define models as object classes that can store
# multiple model layers

class Model(nn.Module):

    def __init__(self,in_features,out_features):

        super().__init__()
        self.linear= nn.Linear(in_features,out_features)

# nn.Linear is not refer to model
# it refers to type of neural netwoek layer that is employed here.
# Linear layer is fully connected or dense layer.
# our model containt linear layers, convolutional layers,pooling
# layers etc.

    def forward(self,x):
        y_pred=self.linear(x)
        return y_pred

torch.manual_seed(59)
model= Model(1,1)
print(model)

print(" ")

print(model.linear.weight)

print(" ")

print(model.linear.bias)

for name,param in model.named_parameters():

    #named_parameters inherited from of nn.module
    #intialized at  super().__init__()

    print(name,'\t',param.item())

# let's see result when we pass a tensor into the model

x = torch.tensor([2.0])
print(model.forward(x))

# if you add x=2 in f(x)= (0.1060)(2.0)+(0.9638)=1.1758
# f(x)= 2.0 * weight + bias

# now lets see how it perform for a variety of x values

x1 = np.linspace(0.0,50.0,50)
x1

# what would our model predict y given value of X1

w1= 0.1059
b1= 0.9637

y1= w1*x1 + b1
y1

plt.scatter(X.numpy(),y.numpy())
plt.plot(x1,y1,'r')

# So if we do not set any loss function or optimization at all,
# model will essentially just guessing off its first random
# weight and first random bias perform really porly.

# So how do we actually model learn or at least try to optimize?

# So what we need to do is we need to set loss function
# So we could write our own function to apply mean squared error
# but neural network function within PyTorch has it


# criterion or criteria on which you are evaluating your function
# or your network's perfomance.

# we are evaluating based on mean squared error loss

crieterion = nn.MSELoss()

# Now we need to set an optimization, and this is
# where we are going to use
# Stochastic gradient descent with an applied learning rate.

# Learning tells the optimizer how much to adjust each parameter
# on the next round of calculations

# If you have two large of a step , then you run the risk
# of overshooting the minimum.

# If you choose too small, its gonna take really long time
# to converge.

# So often with more complex networks, we have to play around
# the learning rate a little bit.

# In our case it is Linear Regression.

# SGD:- Stochastic Gradient Descent

optimizer=torch.optim.SGD(model.parameters(),lr=0.001)

optimizer

# train the model

# select the Epoch

# Epoch:- is a single pass through the entire dataset

# So we want to pick a sufficiently large number of epochs
# to reach a plateau close to our known parameters
# where we actually set a known weight of 2 and bias of 1.

# y= 2 *X +1

# Choose a reasonable number of passes.

# keep track of the loss to find out.

epoch = 50

losses = []

for i in range(epoch):

    i= i+1

    # predicting on the forward pass
    y_pred= model.forward(X)

    # caculate loss(error)
    loss= crieterion(y_pred,y)

    # record error
    losses.append(loss)

    print(f"epcoh {i} \
    loss: {loss.item()} \
    weight : {model.linear.weight.item()} \
    bias: {model.linear.bias.item()}")

    # adjust weight and bias
    # we will caculate loss and the print this results
    # the gradient accumulate with every backpropagation
    # So to prevent compounding we need to reset the stored
    # gradient for each new epoch

    optimizer.zero_grad()

    loss.backward()
    #backpropagation step

    optimizer.step()
    #updating hyperparameter
    # in this case it is single weight and single bias

#plot

#plt.scatter(X.numpy(),y.numpy())

plt.plot(range(epoch),[loss.detach().numpy() for loss in losses])
plt.ylabel("MSE Loss")
plt.xlabel("Epoch")

x = np.linspace(0.0,50.0,50)

current_weight=model.linear.weight.item()

current_bias= model.linear.bias.item()

predicted_y= current_weight*x + current_bias
predicted_y

plt.scatter(X.numpy(),y.numpy())
plt.plot(x,predicted_y,'r')